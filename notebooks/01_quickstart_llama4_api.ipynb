{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f6112",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Quickstart with Llama 4 API\\n\",\n",
    "    \"\\n\",\n",
    "    \"Welcome to the **Building with Llama 4** course! This notebook provides a comprehensive introduction to Meta's Llama 4 model and its official API.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üéØ Learning Objectives\\n\",\n",
    "    \"- Understand Llama 4's architecture and capabilities\\n\",\n",
    "    \"- Set up and authenticate with the Llama API\\n\",\n",
    "    \"- Perform basic text generation and chat completions\\n\",\n",
    "    \"- Explore the model's multimodal capabilities\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üìö Course Overview\\n\",\n",
    "    \"This is part of the DeepLearning.AI course series taught by Amit Sangani (Senior Director of Partner Engineering at Meta).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### What You'll Build in This Course:\\n\",\n",
    "    \"1. **Quickstart with Llama 4 API** (This notebook)\\n\",\n",
    "    \"2. **Image Grounding** - Object detection and bounding boxes\\n\",\n",
    "    \"3. **Prompt Formatting** - Optimal prompt structures\\n\",\n",
    "    \"4. **Long-Context Processing** - Handle up to 10M tokens\\n\",\n",
    "    \"5. **Prompt Optimization** - Enhanced sentiment analysis\\n\",\n",
    "    \"6. **Synthetic Data Kit** - Generate training datasets\\n\",\n",
    "    \"7. **Multilingual Translator** - 12+ language support\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîß Setup and Installation\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's install the required packages and set up our environment.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages\\n\",\n",
    "    \"!pip install requests python-dotenv pandas numpy matplotlib\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from dotenv import load_dotenv\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load environment variables\\n\",\n",
    "    \"load_dotenv()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Setup complete!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîë API Authentication\\n\",\n",
    "    \"\\n\",\n",
    "    \"Set up your Llama API credentials. Make sure you have your API key from Meta.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Set up API credentials\\n\",\n",
    "    \"# Option 1: From environment variable\\n\",\n",
    "    \"LLAMA_API_KEY = os.getenv('LLAMA_API_KEY')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Option 2: Direct input (not recommended for production)\\n\",\n",
    "    \"# LLAMA_API_KEY = \\\"your_api_key_here\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not LLAMA_API_KEY:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è Warning: LLAMA_API_KEY not found!\\\")\\n\",\n",
    "    \"    print(\\\"Please set your API key in a .env file or as an environment variable\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ API key loaded successfully!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# API endpoint\\n\",\n",
    "    \"API_BASE_URL = \\\"https://api.llama-api.com/chat/completions\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Headers for API requests\\n\",\n",
    "    \"headers = {\\n\",\n",
    "    \"    \\\"Authorization\\\": f\\\"Bearer {LLAMA_API_KEY}\\\",\\n\",\n",
    "    \"    \\\"Content-Type\\\": \\\"application/json\\\"\\n\",\n",
    "    \"}\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üöÄ Your First Llama 4 API Call\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's make our first API call to generate text with Llama 4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def call_llama_api(messages, model=\\\"llama-4\\\", max_tokens=500, temperature=0.7):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Make a call to the Llama API\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        messages: List of message dictionaries\\n\",\n",
    "    \"        model: Model name to use\\n\",\n",
    "    \"        max_tokens: Maximum tokens to generate\\n\",\n",
    "    \"        temperature: Randomness in generation (0-1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        Response from the API\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    payload = {\\n\",\n",
    "    \"        \\\"model\\\": model,\\n\",\n",
    "    \"        \\\"messages\\\": messages,\\n\",\n",
    "    \"        \\\"max_tokens\\\": max_tokens,\\n\",\n",
    "    \"        \\\"temperature\\\": temperature\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        response = requests.post(API_BASE_URL, headers=headers, json=payload)\\n\",\n",
    "    \"        response.raise_for_status()\\n\",\n",
    "    \"        return response.json()\\n\",\n",
    "    \"    except requests.exceptions.RequestException as e:\\n\",\n",
    "    \"        print(f\\\"‚ùå API call failed: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test the API with a simple message\\n\",\n",
    "    \"test_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"Hello! Can you explain what makes Llama 4 special in simple terms?\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîÑ Making API call...\\\")\\n\",\n",
    "    \"response = call_llama_api(test_messages)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(\\\"‚úÖ Success! Here's Llama 4's response:\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ùå API call failed. Please check your API key and connection.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üß† Understanding Llama 4's Architecture\\n\",\n",
    "    \"\\n\",\n",
    "    \"Llama 4 uses a Mixture-of-Experts (MoE) architecture. Let's explore its key features:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Let's ask Llama 4 about its own architecture\\n\",\n",
    "    \"architecture_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"\\\"\\\"Explain the Mixture-of-Experts (MoE) architecture in Llama 4. \\n\",\n",
    "    \"        How does it enable efficient model serving? Keep it technical but accessible.\\\"\\\"\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = call_llama_api(architecture_messages, temperature=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(\\\"üß† Llama 4's Architecture Explanation:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 60)\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üí¨ Chat Completion Examples\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's explore different types of interactions with Llama 4:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 1: Code Generation\\n\",\n",
    "    \"code_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"You are a helpful Python programming assistant.\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"Write a Python function to calculate the Fibonacci sequence up to n terms.\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üíª Code Generation Example:\\\")\\n\",\n",
    "    \"response = call_llama_api(code_messages, temperature=0.2)\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*60 + \\\"\\\\n\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 2: Creative Writing\\n\",\n",
    "    \"creative_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"Write a short story (2-3 paragraphs) about an AI that discovers it can dream.\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìù Creative Writing Example:\\\")\\n\",\n",
    "    \"response = call_llama_api(creative_messages, temperature=0.8)\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*60 + \\\"\\\\n\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example 3: Data Analysis Task\\n\",\n",
    "    \"analysis_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"\\\"\\\"Analyze this sales data and provide insights:\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        Quarter 1: $125,000\\n\",\n",
    "    \"        Quarter 2: $150,000\\n\",\n",
    "    \"        Quarter 3: $135,000\\n\",\n",
    "    \"        Quarter 4: $175,000\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        What trends do you see and what recommendations would you make?\\\"\\\"\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Data Analysis Example:\\\")\\n\",\n",
    "    \"response = call_llama_api(analysis_messages, temperature=0.4)\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*60 + \\\"\\\\n\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéõÔ∏è Parameter Exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's experiment with different parameters to understand their effects:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test different temperature values\\n\",\n",
    "    \"prompt = \\\"Write a one-sentence description of artificial intelligence.\\\"\\n\",\n",
    "    \"temperatures = [0.1, 0.5, 0.9]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üå°Ô∏è Temperature Comparison:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for temp in temperatures:\\n\",\n",
    "    \"    messages = [{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}]\\n\",\n",
    "    \"    response = call_llama_api(messages, temperature=temp, max_tokens=100)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if response:\\n\",\n",
    "    \"        print(f\\\"\\\\nüî• Temperature {temp}:\\\")\\n\",\n",
    "    \"        print(response['choices'][0]['message']['content'])\\n\",\n",
    "    \"        print(\\\"-\\\" * 40)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìà API Response Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze the structure of API responses and track usage:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Make a call and analyze the full response structure\\n\",\n",
    "    \"analysis_messages = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"Explain quantum computing in 3 bullet points.\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = call_llama_api(analysis_messages)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if response:\\n\",\n",
    "    \"    print(\\\"üìã Full API Response Structure:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 40)\\n\",\n",
    "    \"    print(json.dumps(response, indent=2))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract key information\\n\",\n",
    "    \"    print(\\\"\\\\nüìä Response Analysis:\\\")\\n\",\n",
    "    \"    print(f\\\"Model used: {response.get('model', 'N/A')}\\\")\\n\",\n",
    "    \"    print(f\\\"Total tokens: {response.get('usage', {}).get('total_tokens', 'N/A')}\\\")\\n\",\n",
    "    \"    print(f\\\"Prompt tokens: {response.get('usage', {}).get('prompt_tokens', 'N/A')}\\\")\\n\",\n",
    "    \"    print(f\\\"Completion tokens: {response.get('usage', {}).get('completion_tokens', 'N/A')}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüí¨ Generated Response:\\\")\\n\",\n",
    "    \"    print(response['choices'][0]['message']['content'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîÑ Conversation Flow\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's demonstrate how to maintain context in a multi-turn conversation:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Multi-turn conversation example\\n\",\n",
    "    \"conversation = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"You are a helpful AI assistant specialized in machine learning.\\\"\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"What's the difference between supervised and unsupervised learning?\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üí¨ Multi-turn Conversation:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# First response\\n\",\n",
    "    \"response1 = call_llama_api(conversation, temperature=0.3)\\n\",\n",
    "    \"if response1:\\n\",\n",
    "    \"    assistant_response = response1['choices'][0]['message']['content']\\n\",\n",
    "    \"    print(\\\"ü§ñ Assistant:\\\", assistant_response)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add the response to conversation history\\n\",\n",
    "    \"    conversation.append({\\n\",\n",
    "    \"        \\\"role\\\": \\\"assistant\\\",\\n\",\n",
    "    \"        \\\"content\\\": assistant_response\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Follow-up question\\n\",\n",
    "    \"    conversation.append({\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": \\\"Can you give me a practical example of each type?\\\"\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nüë§ User: Can you give me a practical example of each type?\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Second response with context\\n\",\n",
    "    \"    response2 = call_llama_api(conversation, temperature=0.3)\\n\",\n",
    "    \"    if response2:\\n\",\n",
    "    \"        print(\\\"ü§ñ Assistant:\\\", response2['choices'][0]['message']['content'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚úÖ Key Takeaways\\n\",\n",
    "    \"\\n\",\n",
    "    \"From this quickstart notebook, you've learned:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **API Setup**: How to authenticate and make calls to Llama 4\\n\",\n",
    "    \"2. **Basic Usage**: Text generation and chat completions\\n\",\n",
    "    \"3. **Parameter Control**: Temperature, max_tokens, and their effects\\n\",\n",
    "    \"4. **Conversation Management**: Maintaining context across turns\\n\",\n",
    "    \"5. **Response Analysis**: Understanding API response structure\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üéØ Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"Ready to explore more advanced capabilities? Continue with:\\n\",\n",
    "    \"- **Notebook 2**: Image Grounding and Object Detection\\n\",\n",
    "    \"- **Notebook 3**: Advanced Prompt Formatting\\n\",\n",
    "    \"- **Notebook 4**: Long-Context Processing (10M tokens!)\\n\",\n",
    "    \"- And more!\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üìö Additional Resources\\n\",\n",
    "    \"\\n\",\n",
    "    \"- [Llama API Documentation](https://developers.meta.com/docs/llama/)\\n\",\n",
    "    \"- [DeepLearning.AI Course Platform](https://www.deeplearning.ai/)\\n\",\n",
    "    \"- [Meta AI Research](https://ai.meta.com/)\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"*Happy coding with Llama 4! ü¶ô‚ú®*\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Final test to ensure everything is working\\n\",\n",
    "    \"print(\\\"üéâ Congratulations! You've completed the Llama 4 API Quickstart!\\\")\\n\",\n",
    "    \"print(\\\"üìù Summary of what we covered:\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ API authentication and setup\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ Basic text generation\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ Chat completions\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ Parameter experimentation\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ Multi-turn conversations\\\")\\n\",\n",
    "    \"print(\\\"   ‚úÖ Response analysis\\\")\\n\",\n",
    "    \"print(\\\"\\\\nüöÄ Ready to move on to more advanced topics!\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
